{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d740b9830fa42b18c5391142511ee91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c4142e6d9b14c68a0b64b4b987f59b1",
              "IPY_MODEL_afaf8fe7a1504192b31c385af5b5bb84",
              "IPY_MODEL_529209996b5f44088b746205c83cc4f1"
            ],
            "layout": "IPY_MODEL_c2fe50c44b4f4ab993bb6bad42c6b0c8"
          }
        },
        "5c4142e6d9b14c68a0b64b4b987f59b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_473319636cc2485cb222826b8eac98a6",
            "placeholder": "​",
            "style": "IPY_MODEL_c180ebdca7004d6a9596cd05ac56ac9b",
            "value": "100%"
          }
        },
        "afaf8fe7a1504192b31c385af5b5bb84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09fd42a4fef74168bd0c618c9530bc2c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef309228459b459dabd199ab6b0e3643",
            "value": 3
          }
        },
        "529209996b5f44088b746205c83cc4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2803c1b2297849d39975972302c0ba48",
            "placeholder": "​",
            "style": "IPY_MODEL_0e6bbfbadfd04e3e9654a53e038259d2",
            "value": " 3/3 [00:00&lt;00:00, 79.28it/s]"
          }
        },
        "c2fe50c44b4f4ab993bb6bad42c6b0c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473319636cc2485cb222826b8eac98a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c180ebdca7004d6a9596cd05ac56ac9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09fd42a4fef74168bd0c618c9530bc2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef309228459b459dabd199ab6b0e3643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2803c1b2297849d39975972302c0ba48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e6bbfbadfd04e3e9654a53e038259d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install requirements / Clone repository"
      ],
      "metadata": {
        "id": "01oAbghNO_5h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m4pCvymKOLkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc1fb90-ac83-4003-92d2-a3b1765267cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DecompX' already exists and is not an empty directory.\n",
            "Requirement already satisfied: datasets==1.18.3 in /usr/local/lib/python3.10/dist-packages (1.18.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (1.26.4)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (17.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (0.3.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (0.70.17)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->datasets==1.18.3) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==1.18.3) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==1.18.3) (0.2.0)\n",
            "Requirement already satisfied: transformers==4.18.0 in /usr/local/lib/python3.10/dist-packages (4.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2.32.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (0.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (1.4.2)\n"
          ]
        }
      ],
      "source": [
        "! git clone \"https://github.com/mohsenfayyaz/DecompX\"\n",
        "! pip install datasets==1.18.3\n",
        "! pip install transformers==4.18.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config (Change model and sentence here)"
      ],
      "metadata": {
        "id": "46T7VHabRQeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoTokenizer\n",
        "from DecompX.src.decompx_utils import DecompXConfig\n",
        "from DecompX.src.modeling_bert import BertForSequenceClassification\n",
        "from DecompX.src.modeling_roberta import RobertaForSequenceClassification\n",
        "\n",
        "BERT_MODELS = [\"lyeonii/bert-tiny\", \"lyeonii/bert-mini\", \"lyeonii/bert-small\", \"lyeonii/bert-medium\", \"google-bert/bert-base-uncased\", \"google-bert/bert-large-uncased\"]\n",
        "ROBERTA_MODELS = [\"smallbenchnlp/roberta-small\",\"JackBAI/roberta-medium\",\"FacebookAI/roberta-base\", \"FacebookAI/roberta-large\"]\n",
        "SENTENCES = [\n",
        "    \"A deep and meaningful film.\",\n",
        "    \"a good piece of work more often than not.\",\n",
        "]\n",
        "CONFIGS = {\n",
        "    \"DecompX\":\n",
        "        DecompXConfig(\n",
        "            include_biases=True,\n",
        "            bias_decomp_type=\"absdot\",\n",
        "            include_LN1=True,\n",
        "            include_FFN=True,\n",
        "            FFN_approx_type=\"GeLU_ZO\",\n",
        "            include_LN2=True,\n",
        "            aggregation=\"vector\",\n",
        "            include_classifier_w_pooler=True,\n",
        "            tanh_approx_type=\"ZO\",\n",
        "            output_all_layers=True,\n",
        "            output_attention=None,\n",
        "            output_res1=None,\n",
        "            output_LN1=None,\n",
        "            output_FFN=None,\n",
        "            output_res2=None,\n",
        "            output_encoder=None,\n",
        "            output_aggregated=\"norm\",\n",
        "            output_pooler=\"norm\",\n",
        "            output_classifier=True,\n",
        "        ),\n",
        "}"
      ],
      "metadata": {
        "id": "Uop2VstiQm02"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load corresponding model/tokenizer"
      ],
      "metadata": {
        "id": "NhO3vaddcZ-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name, input_sentences):\n",
        "  model = None\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenized_sentence = tokenizer(input_sentences, return_tensors=\"pt\", padding=True)\n",
        "  batch_lengths = tokenized_sentence['attention_mask'].sum(dim=-1)\n",
        "  if \"roberta\" in model_name:\n",
        "      model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
        "  elif \"bert\" in model_name:\n",
        "      model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "  else:\n",
        "      raise Exception(f\"Not implented model: {model_name}\")\n",
        "  return model, tokenizer, tokenized_sentence, batch_lengths"
      ],
      "metadata": {
        "id": "A9rSOA4gUqWV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute DecompX"
      ],
      "metadata": {
        "id": "zZOpodv_cbUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_decompx(model, tokenizer, tokenized_sentence, batch_lengths):\n",
        "  # logits ~ (8, 2)\n",
        "  # hidden_states ~ (13, 8, 55, 768)\n",
        "  # decompx_last_layer_outputs.aggregated ~ (1, 8, 55, 55)\n",
        "  # decompx_last_layer_outputs.pooler ~ (1, 8, 55)\n",
        "  # decompx_last_layer_outputs.classifier ~ (8, 55, 2)\n",
        "  # decompx_all_layers_outputs.aggregated ~ (12, 8, 55, 55)\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    logits, hidden_states, decompx_last_layer_outputs, decompx_all_layers_outputs = model(\n",
        "        **tokenized_sentence,\n",
        "        output_attentions=False,\n",
        "        return_dict=False,\n",
        "        output_hidden_states=True,\n",
        "        decompx_config=CONFIGS[\"DecompX\"]\n",
        "    )\n",
        "\n",
        "  predictions = torch.argmax(logits, dim=1).cpu().tolist()  # Predicted class\n",
        "  decompx_outputs = {\n",
        "    \"tokens\": [tokenizer.convert_ids_to_tokens(tokenized_sentence[\"input_ids\"][i][:batch_lengths[i]]) for i in range(len(labels))],\n",
        "    \"logits\": logits.cpu().detach().numpy().tolist(),  # (batch, classes)\n",
        "    \"cls\": hidden_states[-1][:, 0, :].cpu().detach().numpy().tolist(),# Last layer & only CLS -> (batch, emb_dim)\n",
        "    \"predictions\": predictions\n",
        "  }\n",
        "\n",
        "  ### decompx_last_layer_outputs.aggregated ~ (1, 8, 55, 55) ###\n",
        "  importance = np.array([g.squeeze().cpu().detach().numpy() for g in decompx_last_layer_outputs.aggregated]).squeeze()  # (batch, seq_len, seq_len)\n",
        "  importance = [importance[j][:batch_lengths[j],:batch_lengths[j]] for j in range(len(importance))]\n",
        "  decompx_outputs[\"importance_last_layer_aggregated\"] = importance\n",
        "\n",
        "  ### decompx_last_layer_outputs.pooler ~ (1, 8, 55) ###\n",
        "  importance = np.array([g.squeeze().cpu().detach().numpy() for g in decompx_last_layer_outputs.pooler]).squeeze()  # (batch, seq_len)\n",
        "  importance = [importance[j][:batch_lengths[j]] for j in range(len(importance))]\n",
        "  decompx_outputs[\"importance_last_layer_pooler\"] = importance\n",
        "\n",
        "  ### decompx_last_layer_outputs.classifier ~ (8, 55, 2) ###\n",
        "  importance = np.array([g.squeeze().cpu().detach().numpy() for g in decompx_last_layer_outputs.classifier]).squeeze()  # (batch, seq_len, classes)\n",
        "  importance = [importance[j][:batch_lengths[j], :] for j in range(len(importance))]\n",
        "  decompx_outputs[\"importance_last_layer_classifier\"] = importance\n",
        "\n",
        "  ### decompx_all_layers_outputs.aggregated ~ (12, 8, 55, 55) ###\n",
        "  importance = np.array([g.squeeze().cpu().detach().numpy() for g in decompx_all_layers_outputs.aggregated])  # (layers, batch, seq_len, seq_len)\n",
        "  importance = np.einsum('lbij->blij', importance)  # (batch, layers, seq_len, seq_len)\n",
        "  importance = [importance[j][:, :batch_lengths[j], :batch_lengths[j]] for j in range(len(importance))]\n",
        "  decompx_outputs[\"importance_all_layers_aggregated\"] = importance\n",
        "\n",
        "  decompx_outputs_df = pd.DataFrame(decompx_outputs)\n",
        "\n",
        "  return decompx_outputs_df, importance"
      ],
      "metadata": {
        "id": "JVg5RXtzS1Vo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "lyOIqbryc9c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_importance(importance, tokenized_text, discrete=False, prefix=\"\", no_cls_sep=False):\n",
        "    \"\"\"\n",
        "    importance: (sent_len)\n",
        "    \"\"\"\n",
        "    if no_cls_sep:\n",
        "        importance = importance[1:-1]\n",
        "        tokenized_text = tokenized_text[1:-1]\n",
        "    importance = importance / np.abs(importance).max() / 1.5  # Normalize\n",
        "    if discrete:\n",
        "        importance = np.argsort(np.argsort(importance)) / len(importance) / 1.6\n",
        "\n",
        "    html = \"<pre style='color:black; padding: 3px;'>\"+prefix\n",
        "    for i in range(len(tokenized_text)):\n",
        "        if importance[i] >= 0:\n",
        "            rgba = matplotlib.colormaps.get_cmap('Greens')(importance[i])   # Wistia\n",
        "        else:\n",
        "            rgba = matplotlib.colormaps.get_cmap('Reds')(np.abs(importance[i]))   # Wistia\n",
        "        text_color = \"color: rgba(255, 255, 255, 1.0); \" if np.abs(importance[i]) > 0.9 else \"\"\n",
        "        color = f\"background-color: rgba({rgba[0]*255}, {rgba[1]*255}, {rgba[2]*255}, {rgba[3]}); \" + text_color\n",
        "        html += (f\"<span style='\"\n",
        "                 f\"{color}\"\n",
        "                 f\"border-radius: 5px; padding: 3px;\"\n",
        "                 f\"font-weight: {int(800)};\"\n",
        "                 \"'>\")\n",
        "        html += tokenized_text[i].replace('<', \"[\").replace(\">\", \"]\")\n",
        "        html += \"</span> \"\n",
        "    display(HTML(html))\n",
        "#     print(html)\n",
        "    return html\n",
        "\n",
        "def print_preview(model, tokenizer, tokenized_sentence, batch_lengths, labels, idx=0, discrete=False):\n",
        "    NO_CLS_SEP = False\n",
        "    df, _ = compute_decompx(model, tokenizer, tokenized_sentence, batch_lengths)\n",
        "\n",
        "    actual_label = labels[idx]\n",
        "    predicted_label = df[\"predictions\"][idx]\n",
        "    print(f\"Actual Label: {actual_label} (Non-toxic: 0, Toxic: 1)\")\n",
        "    print(f\"Predicted Label: {predicted_label} (Non-toxic: 0, Toxic: 1)\")\n",
        "\n",
        "    for col in [\"importance_last_layer_aggregated\", \"importance_last_layer_classifier\"]:\n",
        "        if col in df and df[col][idx] is not None:\n",
        "            if \"aggregated\" in col:\n",
        "                sentence_importance = df[col].iloc[idx][0, :]\n",
        "            if \"classifier\" in col:\n",
        "                for label in range(df[col].iloc[idx].shape[-1]):\n",
        "                    sentence_importance = df[col].iloc[idx][:, label]\n",
        "                    print_importance(\n",
        "                        sentence_importance,\n",
        "                        df[\"tokens\"].iloc[idx],\n",
        "                        prefix=f\"{col.split('_')[-1]} Label{label}:\".ljust(20),\n",
        "                        no_cls_sep=NO_CLS_SEP,\n",
        "                        discrete=False\n",
        "                    )\n",
        "                break\n",
        "                sentence_importance = df[col].iloc[idx][:, df[\"label\"].iloc[idx]]\n",
        "            if \"pooler\" in col:\n",
        "                sentence_importance = df[col].iloc[idx]\n",
        "            print_importance(\n",
        "                sentence_importance,\n",
        "                df[\"tokens\"].iloc[idx],\n",
        "                prefix=f\"{col.split('_')[-1]}:\".ljust(20),\n",
        "                no_cls_sep=NO_CLS_SEP,\n",
        "                discrete=discrete\n",
        "            )\n",
        "    print(\"------------------------------------\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "rytX6bB6Wd1f"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"shlomihod/civil-comments-wilds\")\n",
        "\n",
        "# Print the size of the dataset\n",
        "for split in dataset:\n",
        "    print(f\"Split: {split}, Size: {len(dataset[split])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "9d740b9830fa42b18c5391142511ee91",
            "5c4142e6d9b14c68a0b64b4b987f59b1",
            "afaf8fe7a1504192b31c385af5b5bb84",
            "529209996b5f44088b746205c83cc4f1",
            "c2fe50c44b4f4ab993bb6bad42c6b0c8",
            "473319636cc2485cb222826b8eac98a6",
            "c180ebdca7004d6a9596cd05ac56ac9b",
            "09fd42a4fef74168bd0c618c9530bc2c",
            "ef309228459b459dabd199ab6b0e3643",
            "2803c1b2297849d39975972302c0ba48",
            "0e6bbfbadfd04e3e9654a53e038259d2"
          ]
        },
        "id": "3rJBGZ-mikwb",
        "outputId": "bfd7226f-d8ba-48d6-b2b9-71f23038fd0b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:No config specified, defaulting to: civil_comments_wilds/all\n",
            "WARNING:datasets.builder:Reusing dataset civil_comments_wilds (/root/.cache/huggingface/datasets/shlomihod___civil_comments_wilds/all/1.0.0/2f0ae0ef8ccaba1f6c1e327b3f16c139daa9332c24bfe2d89ec3c37ea56af8ea)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d740b9830fa42b18c5391142511ee91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split: train, Size: 269038\n",
            "Split: validation, Size: 45180\n",
            "Split: test, Size: 133782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][5:13]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24sC6RurinCJ",
        "outputId": "f628c6b2-78c5-4a49-805b-af2f78549d9f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['JackJohnson5:\\n\"And they say Trump supporters are uneducated!\"\\n\\n-\\n\\nNo, not necessarily uneducated, just too emotionally immature to admit that they\\'ve bought a lie.\\n\\nBeing wrong hurts, so they choose to accept only the information that confirms what they already believe.',\n",
              "  \"Trump could accidently push a button and Canada \\nwould be on it's back with a boot and knife on it's\\nthroat. That will be the outcome of your wise crack.\",\n",
              "  \"CNN fake News Network Jim Acosta is a douchebag the United States has five intelligence agencies and 17 branches there was three agencies said Russia did it CIA FBI NSA like I said he's a douchebag\",\n",
              "  'Are you a Pilgrim?\\nWhy arn\\'t you growing your own veggies? \\nAnd do you haul them in a covered wagon?\\n\\nThankfully it isn\\'t up to \"white man\" to determine what is traditional for the First Peoples of Alaska.\\nBut they try to all the time and it looks pathetic.',\n",
              "  'Any way you put it or say it,  or con-cock it,  word play, semantics, politically correct, this BEHAVIOR is DIS CUSSTING,  GROSS, PUKY,  these people who engage in this kind of BEHAVIOR  are MENTALLY ILL, and they are INFECTING MILLIONS, I hope they all DIE, it would cleanse the GENE POOL.',\n",
              "  'No, he was accused of being a racist white man.',\n",
              "  'why not you ignorant left wing trash! its called engaging with our largest trading country and our neighbor, and it works,, too bad its the left like you that spews the hate , lies, and other such trash talk,,,, you be ashamed and keep crying, we will celebrate trump and work together ,, psst , need a tissue?',\n",
              "  \"The people we elect (and keep electing) in Oregon...sheesh, it's embarrassing...\"],\n",
              " 'label': [1, 0, 1, 1, 1, 0, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visual_evaluation(model_name, examples, labels):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    # Load the model and tokenizer\n",
        "    model, tokenizer, tokenized_sentence, batch_lengths = load_model_and_tokenizer(model_name, examples)\n",
        "\n",
        "    # Evaluate each example\n",
        "    for i in range(len(examples)):\n",
        "        df = print_preview(model, tokenizer, tokenized_sentence, batch_lengths, labels, idx=i)\n",
        "\n",
        "        # Compute accuracy\n",
        "        if df[\"predictions\"][i] == labels[i]:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "\n",
        "    # Print accuracy\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Accuracy for {model_name}: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "jH6EYVuNYJWH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = dataset['train'][10:12]['text']\n",
        "labels = dataset['train'][10:12]['label']"
      ],
      "metadata": {
        "id": "9UMbQZ9BYjic"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in BERT_MODELS + ROBERTA_MODELS:\n",
        "    print(f\"Evaluating Model: {model_name}\")\n",
        "    visual_evaluation(model_name, examples, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "phl5i0bBYetR",
        "outputId": "b67eb942-7a53-4197-d5aa-d0759a16c7ef"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Model: google-bert/bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Label: 0 (Non-toxic: 0, Toxic: 1)\n",
            "Predicted Label: 0 (Non-toxic: 0, Toxic: 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style='color:black; padding: 3px;'>aggregated:         <span style='background-color: rgba(55.0, 160.33333333333334, 85.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>[CLS]</span> <span style='background-color: rgba(130.8235294117647, 202.91764705882355, 130.18823529411765, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>no</span> <span style='background-color: rgba(223.23529411764704, 242.69411764705882, 217.85098039215688, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>,</span> <span style='background-color: rgba(222.29411764705884, 242.31764705882355, 216.8470588235294, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>he</span> <span style='background-color: rgba(221.3529411764706, 241.94117647058823, 215.84313725490196, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>was</span> <span style='background-color: rgba(120.94117647058823, 198.30588235294118, 122.06274509803922, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>accused</span> <span style='background-color: rgba(230.6235294117647, 245.6313725490196, 225.89411764705883, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(222.29411764705884, 242.31764705882355, 216.8470588235294, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>being</span> <span style='background-color: rgba(230.05882352941177, 245.41176470588235, 225.23529411764704, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(143.52941176470588, 208.8470588235294, 140.63529411764705, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>racist</span> <span style='background-color: rgba(214.76470588235296, 239.30588235294118, 208.8156862745098, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>white</span> <span style='background-color: rgba(222.29411764705884, 242.31764705882355, 216.8470588235294, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>man</span> <span style='background-color: rgba(219.47058823529412, 241.18823529411767, 213.83529411764707, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> <span style='background-color: rgba(204.41176470588235, 235.16470588235293, 197.77254901960785, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>[SEP]</span> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style='color:black; padding: 3px;'>classifier Label0:  <span style='background-color: rgba(227.0, 47.333333333333336, 39.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>[CLS]</span> <span style='background-color: rgba(88.0, 182.27450980392155, 104.27450980392157, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>no</span> <span style='background-color: rgba(241.35294117647058, 249.80392156862746, 238.41176470588235, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>,</span> <span style='background-color: rgba(254.52941176470588, 235.11764705882354, 225.88235294117646, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>he</span> <span style='background-color: rgba(254.52941176470588, 235.11764705882354, 225.88235294117646, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>was</span> <span style='background-color: rgba(252.98823529411766, 205.28235294117647, 185.21176470588236, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>accused</span> <span style='background-color: rgba(254.1843137254902, 227.87058823529412, 215.52941176470586, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(252.8, 201.8, 180.6, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>being</span> <span style='background-color: rgba(254.65490196078431, 237.7529411764706, 229.64705882352942, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(255.0, 245.0, 240.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>racist</span> <span style='background-color: rgba(245.30588235294118, 251.34117647058824, 243.0235294117647, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>white</span> <span style='background-color: rgba(192.74117647058821, 230.36470588235295, 185.90588235294118, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>man</span> <span style='background-color: rgba(253.5529411764706, 215.72941176470587, 199.0470588235294, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> <span style='background-color: rgba(255.0, 245.0, 240.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>[SEP]</span> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style='color:black; padding: 3px;'>classifier Label1:  <span style='background-color: rgba(227.0, 47.333333333333336, 39.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>[CLS]</span> <span style='background-color: rgba(227.00000000000003, 244.2, 221.86666666666665, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>no</span> <span style='background-color: rgba(240.78823529411767, 249.5843137254902, 237.7529411764706, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>,</span> <span style='background-color: rgba(204.41176470588235, 235.16470588235293, 197.77254901960785, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>he</span> <span style='background-color: rgba(231.18823529411765, 245.85098039215686, 226.55294117647057, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>was</span> <span style='background-color: rgba(252.0, 186.678431372549, 160.6313725490196, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>accused</span> <span style='background-color: rgba(232.31764705882352, 246.2901960784314, 227.87058823529412, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(234.01176470588237, 246.94901960784316, 229.84705882352944, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>being</span> <span style='background-color: rgba(254.81176470588235, 241.0470588235294, 234.35294117647058, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(244.78823529411764, 81.67058823529412, 58.470588235294116, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>racist</span> <span style='background-color: rgba(253.30196078431374, 211.08627450980393, 192.89803921568625, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>white</span> <span style='background-color: rgba(241.91764705882355, 250.0235294117647, 239.07058823529414, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>man</span> <span style='background-color: rgba(243.61176470588236, 250.68235294117648, 241.0470588235294, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> <span style='background-color: rgba(254.5607843137255, 235.7764705882353, 226.8235294117647, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>[SEP]</span> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-6bb6e2dc1845>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mBERT_MODELS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mROBERTA_MODELS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluating Model: {model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvisual_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-17ce7b0a39a2>\u001b[0m in \u001b[0;36mvisual_evaluation\u001b[0;34m(model_name, examples, labels)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Evaluate each example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_preview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Compute accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-0fdf26940f7d>\u001b[0m in \u001b[0;36mprint_preview\u001b[0;34m(model, tokenizer, tokenized_sentence, batch_lengths, labels, idx, discrete)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_preview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscrete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mNO_CLS_SEP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_decompx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mactual_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-73ea0ae59d7a>\u001b[0m in \u001b[0;36mcompute_decompx\u001b[0;34m(model, tokenizer, tokenized_sentence, batch_lengths)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     logits, hidden_states, decompx_last_layer_outputs, decompx_all_layers_outputs = model(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DecompX/src/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, decompx_config)\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   2099\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DecompX/src/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, decompx_config)\u001b[0m\n\u001b[1;32m   1467\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m         )\n\u001b[0;32m-> 1469\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1470\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DecompX/src/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, decompx_config)\u001b[0m\n\u001b[1;32m    932\u001b[0m                 )\n\u001b[1;32m    933\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    935\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                     \u001b[0maggregated_encoder_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DecompX/src/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attribution_vectors, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, decompx_config)\u001b[0m\n\u001b[1;32m    780\u001b[0m                 \u001b[0maccumulated_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m                 \u001b[0mtransformed_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bhsqv,dhv->bhsqd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecomposed_value_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheadmixing_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m                 weighted_layer = torch.einsum('bhks,bhsqd->bhkqd', attention_probs,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}