{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flax\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\charley\\Desktop\\ml-stuf\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    FlaxAutoModelForSequenceClassification\n",
    ")\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Enable CUDA optimizations\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "os.environ['FORCE_SAVE_BIN'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicSpansAnalyzer:\n",
    "    def __init__(self, model_name: str, dataset_name: str = 'heegyu/toxic-spans'):\n",
    "        \"\"\"\n",
    "        Initialize the ToxicSpansAnalyzer with a specific model and dataset.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = load_dataset(dataset_name)\n",
    "        \n",
    "        # Initialize tokenizer with optimized settings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            # model_name,\n",
    "            \"FacebookAI/roberta-base\",\n",
    "            use_fast=True,\n",
    "            model_max_length=256\n",
    "        )\n",
    "        \n",
    "        self.model = None\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def compute_metrics(self, eval_pred: EvalPrediction) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute evaluation metrics for the model.\n",
    "        \"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "    def preprocess_dataset(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Preprocess the dataset for training.\n",
    "        \"\"\"\n",
    "        def preprocess_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"text_of_post\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=128,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=None\n",
    "            )\n",
    "        \n",
    "        def preprocess_labels(examples):\n",
    "            examples[\"labels\"] = examples[\"toxic\"]\n",
    "            return examples\n",
    "        \n",
    "        logger.info(\"Starting dataset preprocessing...\")\n",
    "        \n",
    "        tokenized_datasets = self.dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=4\n",
    "        )\n",
    "        \n",
    "        tokenized_datasets = tokenized_datasets.map(\n",
    "            preprocess_labels,\n",
    "            batched=True,\n",
    "            num_proc=4\n",
    "        )\n",
    "        \n",
    "        columns_to_remove = [\n",
    "            \"text_of_post\", \"toxic\", \"probability\", \"position\",\n",
    "            \"type\", \"support\", \"position_probability\"\n",
    "        ]\n",
    "        tokenized_datasets = tokenized_datasets.remove_columns(columns_to_remove)\n",
    "        \n",
    "        tokenized_datasets.set_format(\"torch\")\n",
    "        \n",
    "        logger.info(\"Dataset preprocessing completed\")\n",
    "        return tokenized_datasets\n",
    "\n",
    "    def save_models(self, output_dir: str, hyperparams: Dict):\n",
    "        \"\"\"\n",
    "        Save the model in multiple formats (PyTorch, TensorFlow, and Flax) \n",
    "        along with hyperparameters and evaluation results.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Saving models to {output_dir}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save hyperparameters and evaluation results\n",
    "        hyperparams_path = os.path.join(output_dir, \"hyperparameters.json\")\n",
    "        with open(hyperparams_path, 'w') as f:\n",
    "            json.dump(hyperparams, f, indent=4)\n",
    "        \n",
    "        # Save evaluation results\n",
    "        eval_results_path = os.path.join(output_dir, \"eval_results.json\")\n",
    "        with open(eval_results_path, 'w') as f:\n",
    "            json.dump(self.last_eval_results, f, indent=4)\n",
    "\n",
    "        # Save PyTorch model\n",
    "        pytorch_dir = os.path.join(output_dir, \"pytorch\")\n",
    "        os.makedirs(pytorch_dir, exist_ok=True)\n",
    "        # Save explicitly as a .bin file\n",
    "        self.model = self.model.cpu()\n",
    "        torch.save(self.model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        self.model.save_pretrained(pytorch_dir)\n",
    "\n",
    "        # Save TensorFlow model\n",
    "        try:\n",
    "            tf_dir = os.path.join(output_dir, \"tensorflow\")\n",
    "            os.makedirs(tf_dir, exist_ok=True)\n",
    "            tf_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "                self.model_name, from_pt=True\n",
    "            )\n",
    "            tf_model.save_pretrained(tf_dir)\n",
    "            logger.info(\"TensorFlow model saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save TensorFlow model: {str(e)}\")\n",
    "\n",
    "        # Save Flax model\n",
    "        try:\n",
    "            flax_dir = os.path.join(output_dir, \"flax\")\n",
    "            os.makedirs(flax_dir, exist_ok=True)\n",
    "            flax_model = FlaxAutoModelForSequenceClassification.from_pretrained(\n",
    "                self.model_name, from_pt=True\n",
    "            )\n",
    "            flax_model.save_pretrained(flax_dir)\n",
    "            logger.info(\"Flax model saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save Flax model: {str(e)}\")\n",
    "\n",
    "        logger.info(\"All models saved successfully.\")\n",
    "\n",
    "    def fine_tune(self, output_dir: str, hyperparams: Dict = None):\n",
    "        \"\"\"\n",
    "        Fine-tune the model with hyperparameter configuration and save results.\n",
    "        \"\"\"\n",
    "        if hyperparams is None:\n",
    "            hyperparams = {'learning_rate': 2e-5, 'batch_size': 32, 'num_epochs': 3}\n",
    "        \n",
    "        tokenized_datasets = self.preprocess_dataset()\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=2\n",
    "        ).to(self.device)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=hyperparams['learning_rate'],\n",
    "            per_device_train_batch_size=hyperparams['batch_size'],\n",
    "            per_device_eval_batch_size=hyperparams['batch_size'] * 2,\n",
    "            num_train_epochs=hyperparams['num_epochs'],\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f\"{output_dir}/logs\",\n",
    "            logging_steps=10,\n",
    "            save_total_limit=2,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            gradient_checkpointing=True,\n",
    "            dataloader_num_workers=4,\n",
    "            dataloader_pin_memory=True,\n",
    "            push_to_hub=False\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        logger.info(f\"Starting training with hyperparameters: {hyperparams}\")\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Evaluate the model\n",
    "        logger.info(\"Evaluating model...\")\n",
    "        eval_results = trainer.evaluate()\n",
    "        self.last_eval_results = eval_results\n",
    "    \n",
    "        return train_result, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(\n",
    "    model_name: str,\n",
    "    learning_rates: List[float],\n",
    "    batch_sizes: List[int],\n",
    "    base_output_dir: str\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Perform a hyperparameter search to find the best configuration.\n",
    "    \"\"\"\n",
    "    best_f1 = 0\n",
    "    best_config = None\n",
    "    best_results = None\n",
    "    \n",
    "    # Create a list to track all configurations and their performance\n",
    "    all_configurations = []\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            logger.info(f\"Testing learning rate: {lr}, batch size: {bs}\")\n",
    "            \n",
    "            try:\n",
    "                # Clear CUDA cache if available\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # Create analyzer and preprocess dataset\n",
    "                analyzer = ToxicSpansAnalyzer(model_name)\n",
    "                tokenized_datasets = analyzer.preprocess_dataset()\n",
    "                \n",
    "                # Prepare hyperparameters for this run\n",
    "                hyperparams = {\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': bs,\n",
    "                    'num_epochs': 3,\n",
    "                    'model_name': model_name\n",
    "                }\n",
    "                \n",
    "                # Set output directory for this specific configuration\n",
    "                output_dir = os.path.join(\n",
    "                    base_output_dir, \n",
    "                    f\"{model_name.replace('/', '_')}_lr{lr}_bs{bs}\"\n",
    "                )\n",
    "                \n",
    "                # Fine-tune and evaluate\n",
    "                _, eval_results = analyzer.fine_tune(\n",
    "                    output_dir=output_dir, \n",
    "                    hyperparams=hyperparams\n",
    "                )\n",
    "                \n",
    "                # Extract F1 score\n",
    "                f1_score = eval_results.get(\"eval_f1\", 0)\n",
    "                \n",
    "                # Track all configurations\n",
    "                configuration_result = {\n",
    "                    'hyperparams': hyperparams,\n",
    "                    'f1_score': f1_score,\n",
    "                    'output_dir': output_dir\n",
    "                }\n",
    "                all_configurations.append(configuration_result)\n",
    "                \n",
    "                # Update best configuration if current is better\n",
    "                if f1_score > best_f1:\n",
    "                    best_f1 = f1_score\n",
    "                    best_config = hyperparams\n",
    "                    best_results = eval_results\n",
    "                    \n",
    "                    # Clean up previous best model directory\n",
    "                    best_model_dir = os.path.join(base_output_dir, \"best_model\")\n",
    "                    if os.path.exists(best_model_dir):\n",
    "                        shutil.rmtree(best_model_dir)\n",
    "                    \n",
    "                    # Save the best model with multiple formats\n",
    "                    os.makedirs(best_model_dir, exist_ok=True)\n",
    "                    analyzer.save_models(best_model_dir, hyperparams)\n",
    "                    \n",
    "                    logger.info(f\"New best model saved. F1 Score: {best_f1}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in hyperparameter search for {model_name} (LR:{lr}, BS:{bs}): {str(e)}\")\n",
    "    \n",
    "    # Log and save all configurations for reference\n",
    "    config_log_path = os.path.join(base_output_dir, \"all_configurations.json\")\n",
    "    with open(config_log_path, 'w') as f:\n",
    "        json.dump(all_configurations, f, indent=4)\n",
    "    \n",
    "    logger.info(f\"Best F1 Score: {best_f1}\")\n",
    "    logger.info(f\"Best Configuration: {best_config}\")\n",
    "    \n",
    "    return best_config, best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_model_to_huggingface(\n",
    "    model_path: str, \n",
    "    repo_name: str, \n",
    "    username: str = None,  # Pass username directly\n",
    "    organization: str = None, \n",
    "    private: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload a fine-tuned model to Hugging Face Model Hub.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the local model directory\n",
    "        repo_name (str): Name of the repository to create/update\n",
    "        username (str, optional): Username for upload if not using organization\n",
    "        organization (str, optional): Organization to upload under\n",
    "        private (bool, optional): Whether the repository should be private. Defaults to False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Hugging Face API\n",
    "        api = HfApi()\n",
    "        \n",
    "        # Determine the full repository name\n",
    "        if username:\n",
    "            full_repo_name = f\"{username}/{repo_name}\"\n",
    "        else:\n",
    "            # If no username or organization provided, raise an error\n",
    "            raise ValueError(\"Must provide either username or organization\")\n",
    "        \n",
    "        # Create the repository if it doesn't exist\n",
    "        try:\n",
    "            api.create_repo(\n",
    "                repo_id=full_repo_name, \n",
    "                private=private,\n",
    "                exist_ok=True  # Won't raise an error if repo already exists\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Repository creation/check failed: {e}\")\n",
    "        \n",
    "        # Upload the entire model directory\n",
    "        api.upload_folder(\n",
    "            folder_path=model_path,\n",
    "            repo_id=full_repo_name,\n",
    "            commit_message=\"Upload fine-tuned toxic spans detection model\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Model successfully uploaded to {full_repo_name}\")\n",
    "        return full_repo_name\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading model to Hugging Face: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_with_hyperparameter_search(\n",
    "    models: List[str],\n",
    "    base_output_dir: str = \"./results\",\n",
    "    learning_rates: List[float] = [1e-5, 2e-5, 3e-5, 5e-5],\n",
    "    batch_sizes: List[int] = [8, 16, 32],\n",
    "    upload_to_hub: bool = False,\n",
    "    organization: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Modified version of run_experiment_with_hyperparameter_search \n",
    "    that includes optional model hub upload.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        logger.info(f\"Processing model: {model_name}\")\n",
    "        \n",
    "        # Prepare model-specific output directory\n",
    "        model_output_dir = os.path.join(base_output_dir, model_name.replace(\"/\", \"_\"))\n",
    "        \n",
    "        try:\n",
    "            # Perform hyperparameter search\n",
    "            best_config, best_results = hyperparameter_search(\n",
    "                model_name=model_name,\n",
    "                learning_rates=learning_rates,\n",
    "                batch_sizes=batch_sizes,\n",
    "                base_output_dir=model_output_dir\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            results[model_name] = {\n",
    "                \"best_configuration\": best_config,\n",
    "                \"best_results\": best_results\n",
    "            }\n",
    "            \n",
    "            # Optional: Upload to Hugging Face Model Hub\n",
    "            if upload_to_hub:\n",
    "                best_model_dir = os.path.join(base_output_dir, \"best_model\")\n",
    "                \n",
    "                # Create a descriptive repo name\n",
    "                repo_name = f\"toxic-spans-{model_name.replace('/', '-')}\"\n",
    "                \n",
    "                # Upload the model\n",
    "                uploaded_repo = upload_model_to_huggingface(\n",
    "                    best_model_dir, \n",
    "                    repo_name, \n",
    "                    username='charleyisballer',\n",
    "                    private=False  # Set to False if you want a public repo\n",
    "                )\n",
    "                \n",
    "                # Add uploaded repo information to results\n",
    "                if uploaded_repo:\n",
    "                    results[model_name][\"uploaded_repo\"] = uploaded_repo\n",
    "            \n",
    "            logger.info(f\"Best configuration for {model_name}: {best_config}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing model {model_name}: {str(e)}\")\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting RoBERTa experiments...\n",
      "INFO:__main__:Processing model: smallbenchnlp/roberta-small\n",
      "INFO:__main__:Testing learning rate: 0.001, batch size: 8\n",
      "INFO:__main__:Using device: cpu\n",
      "INFO:__main__:Starting dataset preprocessing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ac56fbda6c4aef9736e2434663cb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d9cc955df541da9274f97dfcd61fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fbe7221ceb40c58fdaf8da24b16632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deaee3e7c23a4fef808d4d79761db12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dataset preprocessing completed\n",
      "INFO:__main__:Starting dataset preprocessing...\n",
      "INFO:__main__:Dataset preprocessing completed\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at smallbenchnlp/roberta-small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\charley\\Desktop\\ml-stuf\\.venv\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\charley\\AppData\\Local\\Temp\\ipykernel_2728\\95513734.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "INFO:__main__:Starting training with hyperparameters: {'learning_rate': 0.001, 'batch_size': 8, 'num_epochs': 3, 'model_name': 'smallbenchnlp/roberta-small'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8864787587d4eb89cc2cc6d723c47b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.001, BS:8): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 0.001, batch size: 16\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.001, BS:16): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 0.0001, batch size: 8\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.0001, BS:8): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 0.0001, batch size: 16\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.0001, BS:16): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 1e-05, batch size: 8\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:1e-05, BS:8): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 1e-05, batch size: 16\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:1e-05, BS:16): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 0.01, batch size: 8\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.01, BS:8): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 0.01, batch size: 16\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.01, BS:16): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Best F1 Score: 0\n",
      "INFO:__main__:Best Configuration: None\n",
      "INFO:__main__:Best configuration for smallbenchnlp/roberta-small: None\n",
      "INFO:__main__:Starting RoBERTa experiments...\n",
      "INFO:__main__:Processing model: smallbenchnlp/roberta-small\n",
      "INFO:__main__:Testing learning rate: 0.001, batch size: 8\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.001, BS:8): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 0.001, batch size: 16\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.001, BS:16): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 0.0001, batch size: 8\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.0001, BS:8): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 0.0001, batch size: 16\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.0001, BS:16): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 1e-05, batch size: 8\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:1e-05, BS:8): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 1e-05, batch size: 16\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:1e-05, BS:16): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 0.01, batch size: 8\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.01, BS:8): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Testing learning rate: 0.01, batch size: 16\n",
      "ERROR:__main__:Error in hyperparameter search for smallbenchnlp/roberta-small (LR:0.01, BS:16): CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "INFO:__main__:Best F1 Score: 0\n",
      "INFO:__main__:Best Configuration: None\n",
      "INFO:__main__:Best configuration for smallbenchnlp/roberta-small: None\n",
      "INFO:__main__:Hyperparameter search completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading model to Hugging Face: Provided path: 'C:\\Users\\charley\\Desktop\\ml-stuf\\toxic-span-results\\roberta\\best_model' is not a directory\n",
      "Error uploading model to Hugging Face: Provided path: 'C:\\Users\\charley\\Desktop\\ml-stuf\\toxic-span-results\\roberta\\best_model' is not a directory\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define models and hyperparameters\n",
    "    # BERT_MODELS = [\n",
    "    #     \"lyeonii/bert-tiny\",\n",
    "    #     # \"lyeonii/bert-small\",\n",
    "    #     # \"lyeonii/bert-medium\",\n",
    "    #     # \"google-bert/bert-base-uncased\",\n",
    "    #     # \"google-bert/bert-large-uncased\",\n",
    "    #     # \"lyeonii/bert-mini\"\n",
    "    # ]\n",
    "    \n",
    "    ROBERTA_MODELS = [\n",
    "        \"smallbenchnlp/roberta-small\",\n",
    "        # \"JackBAI/roberta-medium\",\n",
    "        # \"FacebookAI/roberta-base\",\n",
    "        # \"FacebookAI/roberta-large\"\n",
    "    ]\n",
    "\n",
    "    # Set up base output directory\n",
    "    base_output_dir = \"./toxic-span-results\"\n",
    "    \n",
    "    # # Run experiments\n",
    "    # logger.info(\"Starting BERT experiments...\")\n",
    "    # bert_results = run_experiment_with_hyperparameter_search(\n",
    "    #     models=BERT_MODELS,\n",
    "    #     base_output_dir=os.path.join(base_output_dir, \"bert\"),\n",
    "    #     learning_rates=[1e-3, 1e-4, 1e-5, 1e-2],\n",
    "    #     batch_sizes=[8, 16],\n",
    "    #     upload_to_hub=True,\n",
    "    # )\n",
    "    \n",
    "    logger.info(\"Starting RoBERTa experiments...\")\n",
    "    roberta_results = run_experiment_with_hyperparameter_search(\n",
    "        models=ROBERTA_MODELS,\n",
    "        base_output_dir=os.path.join(base_output_dir, \"roberta\"),\n",
    "        learning_rates=[1e-3, 1e-4, 1e-5, 1e-2],\n",
    "        batch_sizes=[8, 16],\n",
    "        upload_to_hub=True,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting RoBERTa experiments...\")\n",
    "    roberta_results = run_experiment_with_hyperparameter_search(\n",
    "        models=ROBERTA_MODELS,\n",
    "        base_output_dir=os.path.join(base_output_dir, \"roberta\"),\n",
    "        learning_rates=[1e-3, 1e-4, 1e-5, 1e-2],\n",
    "        batch_sizes=[8, 16],\n",
    "        upload_to_hub=True,\n",
    "    )\n",
    "    \n",
    "    # Save overall results\n",
    "    results_path = os.path.join(base_output_dir, \"experiment_results.json\")\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump({\n",
    "            # \"bert_results\": bert_results,\n",
    "            \"roberta_results\": roberta_results\n",
    "        }, f, indent=4)\n",
    "    \n",
    "    logger.info(\"Hyperparameter search completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # Should return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"TensorFlow is using the GPU\")\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        print(f\"Device: {gpu}\")\n",
    "else:\n",
    "    print(\"TensorFlow is not using the GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))  # Should list your GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
