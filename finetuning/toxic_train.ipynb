{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45717dcf",
   "metadata": {},
   "source": [
    "\n",
    "# Model Finetuning Notebook\n",
    "\n",
    "This notebook demonstrates the implementation and finetuning of various LLM models for the analysis of spurious correlations. \n",
    "Below are the steps involved in this project:\n",
    "1. Data Loading and Preprocessing\n",
    "2. Model Definition\n",
    "3. Training and Validation\n",
    "4. Hyperparameter Tuning and Optimization\n",
    "5. Exporting Results\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66e2de",
   "metadata": {},
   "source": [
    "### Package dependencies (ignore if already installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff93eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f29c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flax\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060410e",
   "metadata": {},
   "source": [
    "### Import statements and logging configuration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bf2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    FlaxAutoModelForSequenceClassification\n",
    ")\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Enable CUDA optimizations\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b2c0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1d15a",
   "metadata": {},
   "source": [
    "### Huggingface setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = None # insert HF token here if necessary\n",
    "os.environ['FORCE_SAVE_BIN'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7919edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=None) # use token here if wanting to login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4247aa38",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41bf0fb",
   "metadata": {},
   "source": [
    "### Class definition with default dataset of toxic-spans\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1fdb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicSpansAnalyzer:\n",
    "    def __init__(self, model_name: str, dataset_name: str = 'heegyu/toxic-spans'):\n",
    "        \"\"\"\n",
    "        Initialize the ToxicSpansAnalyzer with a specific model and dataset.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = load_dataset(dataset_name)\n",
    "        \n",
    "        # Initialize tokenizer with optimized settings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            use_fast=True,\n",
    "            model_max_length=256\n",
    "        )\n",
    "        \n",
    "        self.model = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def compute_metrics(self, eval_pred: EvalPrediction) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute evaluation metrics for the model.\n",
    "        \"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "    def preprocess_dataset(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Preprocess the dataset for training.\n",
    "        \"\"\"\n",
    "        def preprocess_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"text_of_post\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=256,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=None\n",
    "            )\n",
    "        \n",
    "        def preprocess_labels(examples):\n",
    "            examples[\"labels\"] = examples[\"toxic\"]\n",
    "            return examples\n",
    "        \n",
    "        logger.info(\"Starting dataset preprocessing...\")\n",
    "        \n",
    "        tokenized_datasets = self.dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=4\n",
    "        )\n",
    "        \n",
    "        tokenized_datasets = tokenized_datasets.map(\n",
    "            preprocess_labels,\n",
    "            batched=True,\n",
    "            num_proc=4\n",
    "        )\n",
    "        \n",
    "        columns_to_remove = [\n",
    "            \"text_of_post\", \"toxic\", \"probability\", \"position\",\n",
    "            \"type\", \"support\", \"position_probability\"\n",
    "        ]\n",
    "        tokenized_datasets = tokenized_datasets.remove_columns(columns_to_remove)\n",
    "        \n",
    "        tokenized_datasets.set_format(\"torch\")\n",
    "        \n",
    "        logger.info(\"Dataset preprocessing completed\")\n",
    "        return tokenized_datasets\n",
    "\n",
    "    def save_models(self, output_dir: str, hyperparams: Dict):\n",
    "        \"\"\"\n",
    "        Save the model in multiple formats (PyTorch, TensorFlow, and Flax) \n",
    "        along with hyperparameters and evaluation results.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Saving models to {output_dir}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save hyperparameters and evaluation results\n",
    "        hyperparams_path = os.path.join(output_dir, \"hyperparameters.json\")\n",
    "        with open(hyperparams_path, 'w') as f:\n",
    "            json.dump(hyperparams, f, indent=4)\n",
    "        \n",
    "        # Save evaluation results\n",
    "        eval_results_path = os.path.join(output_dir, \"eval_results.json\")\n",
    "        with open(eval_results_path, 'w') as f:\n",
    "            json.dump(self.last_eval_results, f, indent=4)\n",
    "\n",
    "        # Save PyTorch model\n",
    "        pytorch_dir = os.path.join(output_dir, \"pytorch\")\n",
    "        os.makedirs(pytorch_dir, exist_ok=True)\n",
    "        # Save explicitly as a .bin file\n",
    "        self.model = self.model.cpu()\n",
    "        torch.save(self.model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        self.model.save_pretrained(pytorch_dir)\n",
    "\n",
    "        # Save TensorFlow model\n",
    "        try:\n",
    "            tf_dir = os.path.join(output_dir, \"tensorflow\")\n",
    "            os.makedirs(tf_dir, exist_ok=True)\n",
    "            tf_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "                self.model_name, from_pt=True\n",
    "            )\n",
    "            tf_model.save_pretrained(tf_dir)\n",
    "            logger.info(\"TensorFlow model saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save TensorFlow model: {str(e)}\")\n",
    "\n",
    "        # Save Flax model\n",
    "        try:\n",
    "            flax_dir = os.path.join(output_dir, \"flax\")\n",
    "            os.makedirs(flax_dir, exist_ok=True)\n",
    "            flax_model = FlaxAutoModelForSequenceClassification.from_pretrained(\n",
    "                self.model_name, from_pt=True\n",
    "            )\n",
    "            flax_model.save_pretrained(flax_dir)\n",
    "            logger.info(\"Flax model saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save Flax model: {str(e)}\")\n",
    "\n",
    "        logger.info(\"All models saved successfully.\")\n",
    "\n",
    "    def fine_tune(self, output_dir: str, hyperparams: Dict = None):\n",
    "        \"\"\"\n",
    "        Fine-tune the model with hyperparameter configuration and save results.\n",
    "        \"\"\"\n",
    "        if hyperparams is None:\n",
    "            hyperparams = {'learning_rate': 2e-5, 'batch_size': 32, 'num_epochs': 3}\n",
    "        \n",
    "        tokenized_datasets = self.preprocess_dataset()\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=2\n",
    "        ).to(self.device)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=hyperparams['learning_rate'],\n",
    "            per_device_train_batch_size=hyperparams['batch_size'],\n",
    "            per_device_eval_batch_size=hyperparams['batch_size'] * 2,\n",
    "            num_train_epochs=hyperparams['num_epochs'],\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f\"{output_dir}/logs\",\n",
    "            logging_steps=10,\n",
    "            save_total_limit=2,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            gradient_checkpointing=True,\n",
    "            dataloader_num_workers=4,\n",
    "            dataloader_pin_memory=True,\n",
    "            push_to_hub=False\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        logger.info(f\"Starting training with hyperparameters: {hyperparams}\")\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Evaluate the model\n",
    "        logger.info(\"Evaluating model...\")\n",
    "        eval_results = trainer.evaluate()\n",
    "        self.last_eval_results = eval_results\n",
    "    \n",
    "        return train_result, eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da67dd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff789db",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning configuration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6dc248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(\n",
    "    model_name: str,\n",
    "    learning_rates: List[float],\n",
    "    batch_sizes: List[int],\n",
    "    base_output_dir: str\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Perform a hyperparameter search to find the best configuration.\n",
    "    \"\"\"\n",
    "    best_f1 = 0\n",
    "    best_config = None\n",
    "    best_results = None\n",
    "    \n",
    "    # Create a list to track all configurations and their performance\n",
    "    all_configurations = []\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            logger.info(f\"Testing learning rate: {lr}, batch size: {bs}\")\n",
    "            \n",
    "            try:\n",
    "                # Clear CUDA cache if available\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # Create analyzer and preprocess dataset\n",
    "                analyzer = ToxicSpansAnalyzer(model_name)\n",
    "                tokenized_datasets = analyzer.preprocess_dataset()\n",
    "                \n",
    "                # Prepare hyperparameters for this run\n",
    "                hyperparams = {\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': bs,\n",
    "                    'num_epochs': 3,\n",
    "                    'model_name': model_name\n",
    "                }\n",
    "                \n",
    "                # Set output directory for this specific configuration\n",
    "                output_dir = os.path.join(\n",
    "                    base_output_dir, \n",
    "                    f\"{model_name.replace('/', '_')}_lr{lr}_bs{bs}\"\n",
    "                )\n",
    "                \n",
    "                # Fine-tune and evaluate\n",
    "                _, eval_results = analyzer.fine_tune(\n",
    "                    output_dir=output_dir, \n",
    "                    hyperparams=hyperparams\n",
    "                )\n",
    "                \n",
    "                # Extract F1 score\n",
    "                f1_score = eval_results.get(\"eval_f1\", 0)\n",
    "                \n",
    "                # Track all configurations\n",
    "                configuration_result = {\n",
    "                    'hyperparams': hyperparams,\n",
    "                    'f1_score': f1_score,\n",
    "                    'output_dir': output_dir\n",
    "                }\n",
    "                all_configurations.append(configuration_result)\n",
    "                \n",
    "                # Update best configuration if current is better\n",
    "                if f1_score > best_f1:\n",
    "                    best_f1 = f1_score\n",
    "                    best_config = hyperparams\n",
    "                    best_results = eval_results\n",
    "                    \n",
    "                    # Clean up previous best model directory\n",
    "                    best_model_dir = os.path.join(base_output_dir, \"best_model\")\n",
    "                    if os.path.exists(best_model_dir):\n",
    "                        shutil.rmtree(best_model_dir)\n",
    "                    \n",
    "                    # Save the best model with multiple formats\n",
    "                    os.makedirs(best_model_dir, exist_ok=True)\n",
    "                    analyzer.save_models(best_model_dir, hyperparams)\n",
    "                    \n",
    "                    logger.info(f\"New best model saved. F1 Score: {best_f1}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in hyperparameter search for {model_name} (LR:{lr}, BS:{bs}): {str(e)}\")\n",
    "    \n",
    "    # Log and save all configurations for reference\n",
    "    config_log_path = os.path.join(base_output_dir, \"all_configurations.json\")\n",
    "    with open(config_log_path, 'w') as f:\n",
    "        json.dump(all_configurations, f, indent=4)\n",
    "    \n",
    "    logger.info(f\"Best F1 Score: {best_f1}\")\n",
    "    logger.info(f\"Best Configuration: {best_config}\")\n",
    "    \n",
    "    return best_config, best_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb606db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f1e43b",
   "metadata": {},
   "source": [
    "### Upload to huggingface if necessary\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e576f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9690f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_model_to_huggingface(\n",
    "    model_path: str, \n",
    "    repo_name: str, \n",
    "    username: str = None,  # Pass username directly\n",
    "    organization: str = None, \n",
    "    private: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload a fine-tuned model to Hugging Face Model Hub.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the local model directory\n",
    "        repo_name (str): Name of the repository to create/update\n",
    "        username (str, optional): Username for upload if not using organization\n",
    "        organization (str, optional): Organization to upload under\n",
    "        private (bool, optional): Whether the repository should be private. Defaults to False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Hugging Face API\n",
    "        api = HfApi()\n",
    "        \n",
    "        # Determine the full repository name\n",
    "        if username:\n",
    "            full_repo_name = f\"{username}/{repo_name}\"\n",
    "        else:\n",
    "            # If no username or organization provided, raise an error\n",
    "            raise ValueError(\"Must provide either username or organization\")\n",
    "        \n",
    "        # Create the repository if it doesn't exist\n",
    "        try:\n",
    "            api.create_repo(\n",
    "                repo_id=full_repo_name, \n",
    "                private=private,\n",
    "                exist_ok=True  # Won't raise an error if repo already exists\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Repository creation/check failed: {e}\")\n",
    "        \n",
    "        # Upload the entire model directory\n",
    "        api.upload_folder(\n",
    "            folder_path=model_path,\n",
    "            repo_id=full_repo_name,\n",
    "            commit_message=\"Upload fine-tuned toxic spans detection model\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Model successfully uploaded to {full_repo_name}\")\n",
    "        return full_repo_name\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading model to Hugging Face: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57616080",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d0336",
   "metadata": {},
   "source": [
    "### Main experiment loop\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d714f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_with_hyperparameter_search(\n",
    "    models: List[str],\n",
    "    base_output_dir: str = \"./results\",\n",
    "    learning_rates: List[float] = [1e-5, 2e-5, 3e-5, 5e-5],\n",
    "    batch_sizes: List[int] = [8, 16, 32],\n",
    "    upload_to_hub: bool = False,\n",
    "    organization: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Modified version of run_experiment_with_hyperparameter_search \n",
    "    that includes optional model hub upload.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        logger.info(f\"Processing model: {model_name}\")\n",
    "        \n",
    "        # Prepare model-specific output directory\n",
    "        model_output_dir = os.path.join(base_output_dir, model_name.replace(\"/\", \"_\"))\n",
    "        \n",
    "        try:\n",
    "            # Perform hyperparameter search\n",
    "            best_config, best_results = hyperparameter_search(\n",
    "                model_name=model_name,\n",
    "                learning_rates=learning_rates,\n",
    "                batch_sizes=batch_sizes,\n",
    "                base_output_dir=model_output_dir\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            results[model_name] = {\n",
    "                \"best_configuration\": best_config,\n",
    "                \"best_results\": best_results\n",
    "            }\n",
    "            \n",
    "            # Optional: Upload to Hugging Face Model Hub\n",
    "            if upload_to_hub:\n",
    "                best_model_dir = os.path.join(base_output_dir, \"best_model\")\n",
    "                \n",
    "                # Create a descriptive repo name\n",
    "                repo_name = f\"toxic-spans-{model_name.replace('/', '-')}\"\n",
    "                \n",
    "                # Upload the model\n",
    "                uploaded_repo = upload_model_to_huggingface(\n",
    "                    best_model_dir, \n",
    "                    repo_name, \n",
    "                    username='charleyisballer',\n",
    "                    private=False  # Set to False if you want a public repo\n",
    "                )\n",
    "                \n",
    "                # Add uploaded repo information to results\n",
    "                if uploaded_repo:\n",
    "                    results[model_name][\"uploaded_repo\"] = uploaded_repo\n",
    "            \n",
    "            logger.info(f\"Best configuration for {model_name}: {best_config}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing model {model_name}: {str(e)}\")\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b03f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define models and hyperparameters\n",
    "    BERT_MODELS = [\n",
    "        \"lyeonii/bert-tiny\",\n",
    "        \"lyeonii/bert-small\",\n",
    "        \"lyeonii/bert-medium\",\n",
    "        \"google-bert/bert-base-uncased\",\n",
    "        \"google-bert/bert-large-uncased\",\n",
    "        \"lyeonii/bert-mini\"\n",
    "    ]\n",
    "    \n",
    "    ROBERTA_MODELS = [\n",
    "        \"smallbenchnlp/roberta-small\",\n",
    "        \"JackBAI/roberta-medium\",\n",
    "        \"FacebookAI/roberta-base\",\n",
    "        \"FacebookAI/roberta-large\"\n",
    "    ]\n",
    "\n",
    "    # Set up base output directory\n",
    "    base_output_dir = \"./toxic-span-results\"\n",
    "    \n",
    "    # Run experiments\n",
    "    logger.info(\"Starting BERT experiments...\")\n",
    "    bert_results = run_experiment_with_hyperparameter_search(\n",
    "        models=BERT_MODELS,\n",
    "        base_output_dir=os.path.join(base_output_dir, \"bert\"),\n",
    "        learning_rates=[1e-3, 1e-4, 1e-5, 1e-2],\n",
    "        batch_sizes=[8, 16],\n",
    "        upload_to_hub=True,\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Starting RoBERTa experiments...\")\n",
    "    roberta_results = run_experiment_with_hyperparameter_search(\n",
    "        models=ROBERTA_MODELS,\n",
    "        base_output_dir=os.path.join(base_output_dir, \"roberta\"),\n",
    "        learning_rates=[1e-3, 1e-4, 1e-5, 1e-2],\n",
    "        batch_sizes=[8, 16],\n",
    "        upload_to_hub=True,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting RoBERTa experiments...\")\n",
    "    roberta_results = run_experiment_with_hyperparameter_search(\n",
    "        models=ROBERTA_MODELS,\n",
    "        base_output_dir=os.path.join(base_output_dir, \"roberta\"),\n",
    "        learning_rates=[1e-3, 1e-4, 1e-5, 1e-2],\n",
    "        batch_sizes=[8, 16],\n",
    "        upload_to_hub=True,\n",
    "    )\n",
    "    \n",
    "    # Save overall results\n",
    "    results_path = os.path.join(base_output_dir, \"experiment_results.json\")\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump({\n",
    "            \"bert_results\": bert_results,\n",
    "            \"roberta_results\": roberta_results\n",
    "        }, f, indent=4)\n",
    "    \n",
    "    logger.info(\"Hyperparameter search completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
