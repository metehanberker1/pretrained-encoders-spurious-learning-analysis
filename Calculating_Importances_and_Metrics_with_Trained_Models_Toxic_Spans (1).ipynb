{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install requirements / Clone repository"
      ],
      "metadata": {
        "id": "01oAbghNO_5h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m4pCvymKOLkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b581d2-292a-4064-d6e7-c345764f72a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DecompX' already exists and is not an empty directory.\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers==4.18.0 in /usr/local/lib/python3.10/dist-packages (4.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2.32.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (0.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (1.4.2)\n"
          ]
        }
      ],
      "source": [
        "! git clone \"https://github.com/mohsenfayyaz/DecompX\"\n",
        "! pip install -U datasets\n",
        "! pip install transformers==4.18.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "2df8He_AgcJd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config (Change model and sentence here)"
      ],
      "metadata": {
        "id": "46T7VHabRQeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoTokenizer\n",
        "from DecompX.src.decompx_utils import DecompXConfig\n",
        "from DecompX.src.modeling_bert import BertForSequenceClassification\n",
        "from DecompX.src.modeling_roberta import RobertaForSequenceClassification\n",
        "\n",
        "BERT_MODELS = [\"lyeonii/bert-tiny\", \"lyeonii/bert-mini\", \"lyeonii/bert-small\", \"lyeonii/bert-medium\", \"google-bert/bert-base-uncased\", \"google-bert/bert-large-uncased\"]\n",
        "ROBERTA_MODELS = [\"smallbenchnlp/roberta-small\",\"JackBAI/roberta-medium\",\"FacebookAI/roberta-base\", \"FacebookAI/roberta-large\"]\n",
        "SENTENCES = [\n",
        "    \"A deep and meaningful film.\",\n",
        "    \"a good piece of work more often than not.\",\n",
        "]\n",
        "CONFIGS = {\n",
        "    \"DecompX\":\n",
        "        DecompXConfig(\n",
        "            include_biases=True,\n",
        "            bias_decomp_type=\"absdot\",\n",
        "            include_LN1=True,\n",
        "            include_FFN=True,\n",
        "            FFN_approx_type=\"GeLU_ZO\",\n",
        "            include_LN2=True,\n",
        "            aggregation=\"vector\",\n",
        "            include_classifier_w_pooler=True,\n",
        "            tanh_approx_type=\"ZO\",\n",
        "            output_all_layers=True,\n",
        "            output_attention=None,\n",
        "            output_res1=None,\n",
        "            output_LN1=None,\n",
        "            output_FFN=None,\n",
        "            output_res2=None,\n",
        "            output_encoder=None,\n",
        "            output_aggregated=\"norm\",\n",
        "            output_pooler=\"norm\",\n",
        "            output_classifier=True,\n",
        "        ),\n",
        "}"
      ],
      "metadata": {
        "id": "Uop2VstiQm02"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load corresponding model/tokenizer"
      ],
      "metadata": {
        "id": "NhO3vaddcZ-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name, input_sentences):\n",
        "  model = None\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenized_sentence = tokenizer(input_sentences, return_tensors=\"pt\", padding=True)\n",
        "  batch_lengths = tokenized_sentence['attention_mask'].sum(dim=-1)\n",
        "  if \"roberta\" in model_name:\n",
        "      model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
        "  elif \"bert\" in model_name:\n",
        "      model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "  else:\n",
        "      raise Exception(f\"Not implented model: {model_name}\")\n",
        "  return model, tokenizer, tokenized_sentence, batch_lengths"
      ],
      "metadata": {
        "id": "A9rSOA4gUqWV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute DecompX"
      ],
      "metadata": {
        "id": "zZOpodv_cbUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_decompx(model, tokenizer, tokenized_sentence, batch_lengths):\n",
        "  # logits ~ (8, 2)\n",
        "  # hidden_states ~ (13, 8, 55, 768)\n",
        "  # decompx_last_layer_outputs.aggregated ~ (1, 8, 55, 55)\n",
        "  # decompx_last_layer_outputs.pooler ~ (1, 8, 55)\n",
        "  # decompx_last_layer_outputs.classifier ~ (8, 55, 2)\n",
        "  # decompx_all_layers_outputs.aggregated ~ (12, 8, 55, 55)\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    logits, hidden_states, decompx_last_layer_outputs, decompx_all_layers_outputs = model(\n",
        "        **tokenized_sentence,\n",
        "        output_attentions=False,\n",
        "        return_dict=False,\n",
        "        output_hidden_states=True,\n",
        "        decompx_config=CONFIGS[\"DecompX\"]\n",
        "    )\n",
        "\n",
        "  predictions = torch.argmax(logits, dim=1).cpu().tolist()  # Predicted class\n",
        "  decompx_outputs = {\n",
        "    \"tokens\": [tokenizer.convert_ids_to_tokens(tokenized_sentence[\"input_ids\"][i][:batch_lengths[i]]) for i in range(len(batch_lengths))],\n",
        "    \"logits\": logits.cpu().detach().numpy().tolist(),  # (batch, classes)\n",
        "    \"cls\": hidden_states[-1][:, 0, :].cpu().detach().numpy().tolist(),# Last layer & only CLS -> (batch, emb_dim)\n",
        "    \"predictions\": predictions\n",
        "  }\n",
        "\n",
        "  ### decompx_last_layer_outputs.aggregated ~ (1, 8, 55, 55) ###\n",
        "  importance = np.array([g.squeeze().cpu().detach().numpy() for g in decompx_last_layer_outputs.aggregated]).squeeze()  # (batch, seq_len, seq_len)\n",
        "  importance = [importance[j][:batch_lengths[j],:batch_lengths[j]] for j in range(len(importance))]\n",
        "  decompx_outputs[\"importance_last_layer_aggregated\"] = importance\n",
        "\n",
        "  ### decompx_last_layer_outputs.pooler ~ (1, 8, 55) ###\n",
        "  importance = np.array([g.squeeze().cpu().detach().numpy() for g in decompx_last_layer_outputs.pooler]).squeeze()  # (batch, seq_len)\n",
        "  importance = [importance[j][:batch_lengths[j]] for j in range(len(importance))]\n",
        "  decompx_outputs[\"importance_last_layer_pooler\"] = importance\n",
        "\n",
        "  ### decompx_last_layer_outputs.classifier ~ (8, 55, 2) ###\n",
        "  importance = np.array([g.squeeze().cpu().detach().numpy() for g in decompx_last_layer_outputs.classifier]).squeeze()  # (batch, seq_len, classes) num token in that sentence, classes, use classifier\n",
        "  importance = [importance[j][:batch_lengths[j], :] for j in range(len(importance))]\n",
        "  decompx_outputs[\"importance_last_layer_classifier\"] = importance\n",
        "\n",
        "  ### decompx_all_layers_outputs.aggregated ~ (12, 8, 55, 55) ###\n",
        "  importance = np.array([g.squeeze().cpu().detach().numpy() for g in decompx_all_layers_outputs.aggregated])  # (layers, batch, seq_len, seq_len)\n",
        "  importance = np.einsum('lbij->blij', importance)  # (batch, layers, seq_len, seq_len)\n",
        "  importance = [importance[j][:, :batch_lengths[j], :batch_lengths[j]] for j in range(len(importance))]\n",
        "  decompx_outputs[\"importance_all_layers_aggregated\"] = importance\n",
        "\n",
        "  decompx_outputs_df = pd.DataFrame(decompx_outputs)\n",
        "\n",
        "  return decompx_outputs_df, importance"
      ],
      "metadata": {
        "id": "JVg5RXtzS1Vo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "lyOIqbryc9c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_importance(importance, tokenized_text, discrete=False, prefix=\"\", no_cls_sep=False):\n",
        "    \"\"\"\n",
        "    importance: (sent_len)\n",
        "    \"\"\"\n",
        "    if no_cls_sep:\n",
        "        importance = importance[1:-1]\n",
        "        tokenized_text = tokenized_text[1:-1]\n",
        "    importance = importance / np.abs(importance).max() / 1.5  # Normalize\n",
        "    if discrete:\n",
        "        importance = np.argsort(np.argsort(importance)) / len(importance) / 1.6\n",
        "\n",
        "    html = \"<pre style='color:black; padding: 3px;'>\"+prefix\n",
        "    for i in range(len(tokenized_text)):\n",
        "        if importance[i] >= 0:\n",
        "            rgba = matplotlib.colormaps.get_cmap('Greens')(importance[i])   # Wistia\n",
        "        else:\n",
        "            rgba = matplotlib.colormaps.get_cmap('Reds')(np.abs(importance[i]))   # Wistia\n",
        "        text_color = \"color: rgba(255, 255, 255, 1.0); \" if np.abs(importance[i]) > 0.9 else \"\"\n",
        "        color = f\"background-color: rgba({rgba[0]*255}, {rgba[1]*255}, {rgba[2]*255}, {rgba[3]}); \" + text_color\n",
        "        html += (f\"<span style='\"\n",
        "                 f\"{color}\"\n",
        "                 f\"border-radius: 5px; padding: 3px;\"\n",
        "                 f\"font-weight: {int(800)};\"\n",
        "                 \"'>\")\n",
        "        html += tokenized_text[i].replace('<', \"[\").replace(\">\", \"]\")\n",
        "        html += \"</span> \"\n",
        "    display(HTML(html))\n",
        "#     print(html)\n",
        "    return html\n",
        "\n",
        "def print_preview(model, tokenizer, tokenized_sentence, batch_lengths, idx=0, discrete=False):\n",
        "    NO_CLS_SEP = False\n",
        "    df, _ = compute_decompx(model, tokenizer, tokenized_sentence, batch_lengths)\n",
        "\n",
        "    for col in [\"importance_last_layer_aggregated\", \"importance_last_layer_classifier\"]:\n",
        "        if col in df and df[col][idx] is not None:\n",
        "            if \"aggregated\" in col:\n",
        "                sentence_importance = df[col].iloc[idx][0, :]\n",
        "            if \"classifier\" in col:\n",
        "                for label in range(df[col].iloc[idx].shape[-1]):\n",
        "                    sentence_importance = df[col].iloc[idx][:, label]\n",
        "                    print_importance(\n",
        "                        sentence_importance,\n",
        "                        df[\"tokens\"].iloc[idx],\n",
        "                        prefix=f\"{col.split('_')[-1]} Label{label}:\".ljust(20),\n",
        "                        no_cls_sep=NO_CLS_SEP,\n",
        "                        discrete=False\n",
        "                    )\n",
        "                break\n",
        "                sentence_importance = df[col].iloc[idx][:, df[\"label\"].iloc[idx]]\n",
        "            if \"pooler\" in col:\n",
        "                sentence_importance = df[col].iloc[idx]\n",
        "            print_importance(\n",
        "                sentence_importance,\n",
        "                df[\"tokens\"].iloc[idx],\n",
        "                prefix=f\"{col.split('_')[-1]}:\".ljust(20),\n",
        "                no_cls_sep=NO_CLS_SEP,\n",
        "                discrete=discrete\n",
        "            )\n",
        "    print(\"------------------------------------\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "rytX6bB6Wd1f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('heegyu/toxic-spans')\n",
        "\n",
        "# Print the size of the dataset\n",
        "for split in dataset:\n",
        "    print(f\"Split: {split}, Size: {len(dataset[split])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rJBGZ-mikwb",
        "outputId": "4a768829-12f0-46f2-f7d1-6a6ffa3b8d1b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split: train, Size: 10006\n",
            "Split: test, Size: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visual_evaluation(model_name, examples, labels=None):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    # Load the model and tokenizer\n",
        "    model, tokenizer, tokenized_sentence, batch_lengths = load_model_and_tokenizer(model_name, examples)\n",
        "\n",
        "    # Evaluate each example\n",
        "    for i in range(len(examples)):\n",
        "        df = print_preview(model, tokenizer, tokenized_sentence, batch_lengths, idx=i)\n",
        "\n",
        "        if labels:\n",
        "          # Compute accuracy\n",
        "          if df[\"predictions\"][i] == labels[i]:\n",
        "            correct_predictions += 1\n",
        "          total_predictions += 1\n",
        "\n",
        "    if labels:\n",
        "      # Print accuracy\n",
        "      accuracy = correct_predictions / total_predictions\n",
        "      print(f\"Accuracy for {model_name}: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "jH6EYVuNYJWH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"for model_name in BERT_MODELS + ROBERTA_MODELS:\n",
        "    print(f\"Evaluating Model: {model_name}\")\n",
        "    visual_evaluation(model_name, dataset['train'][10:12]['text_of_post'], dataset['train'][10:12]['toxic'])\"\"\""
      ],
      "metadata": {
        "id": "phl5i0bBYetR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8483bfa3-4002-40bd-c01d-587fd3342b18"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for model_name in BERT_MODELS + ROBERTA_MODELS:\\n    print(f\"Evaluating Model: {model_name}\")\\n    visual_evaluation(model_name, dataset[\\'train\\'][10:12][\\'text_of_post\\'], dataset[\\'train\\'][10:12][\\'toxic\\'])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_importance_for_sentences(model, tokenizer, sentences, batch_lengths, labels):\n",
        "    \"\"\"\n",
        "    Compute token importances for a list of sentences.\n",
        "\n",
        "    Args:\n",
        "    - model: The model to use for computation.\n",
        "    - tokenizer: The tokenizer associated with the model.\n",
        "    - sentences: List of input sentences as strings.\n",
        "    - batch_lengths: Lengths of the tokenized batch for the input.\n",
        "    - labels: List of labels corresponding to the sentences.\n",
        "\n",
        "    Returns:\n",
        "    - List of tuples with sentences, tokens, and their importance scores.\n",
        "    \"\"\"\n",
        "    # Tokenize the input sentences\n",
        "    tokenized_sentences = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    batch_lengths = tokenized_sentences[\"attention_mask\"].sum(dim=-1)\n",
        "\n",
        "    # Compute decompositions\n",
        "    decompx_outputs_df, _ = compute_decompx(model, tokenizer, tokenized_sentences, batch_lengths)\n",
        "\n",
        "    results = []\n",
        "    # Process each sentence\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        tokens = decompx_outputs_df[\"tokens\"][idx]  # Retrieve tokens for this sentence\n",
        "        importances = decompx_outputs_df[\"importance_last_layer_classifier\"][idx][:, labels[idx]]  # Importance for the corresponding label\n",
        "        token_importance_pairs = [(token, importance) for token, importance in zip(tokens, importances)]\n",
        "        results.append((sentence, token_importance_pairs))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "zPyV_EFkhl-7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE METRIC"
      ],
      "metadata": {
        "id": "O1HnBy3CCc2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Load examples and labels\n",
        "examples = dataset['train'][0:15]['text_of_post']  # Input sentences\n",
        "labels = dataset['train'][0:15]['toxic']  # Corresponding labels\n",
        "\n",
        "model_name = \"lyeonii/bert-tiny\"\n",
        "model, tokenizer, tokenized_sentence, batch_lengths = load_model_and_tokenizer(model_name, examples)\n",
        "\n",
        "# Parse the 'text' field to extract token groups (phrases) for each example\n",
        "token_groups_list = []\n",
        "for i in range(0, 15):\n",
        "    text_dict = ast.literal_eval(dataset['train'][i]['text'])\n",
        "    tokenized_groups = [tokenizer.tokenize(phrase) for phrase in text_dict.keys()]\n",
        "    token_groups_list.append(tokenized_groups)\n",
        "\n",
        "# Fetch token importances\n",
        "token_importance_results = get_token_importance_for_sentences(model, tokenizer, examples, batch_lengths, labels)\n",
        "\n",
        "# Function to calculate the metric and check token group coverage\n",
        "def calculate_phrase_metric_unordered(token_importance_pairs, token_groups):\n",
        "    \"\"\"\n",
        "    Calculate the metric for token groups in a sentence considering unordered matches.\n",
        "\n",
        "    Args:\n",
        "        token_importance_pairs: List of (token, importance) pairs.\n",
        "        token_groups: List of tokenized groups (phrases) to match.\n",
        "\n",
        "    Returns:\n",
        "        The metric value for the token groups, coverage status, unmatched groups, and unmatched tokens.\n",
        "    \"\"\"\n",
        "    phrase_importance_sum = 0\n",
        "    total_importance = sum(abs(importance) for _, importance in token_importance_pairs)  # Total importance\n",
        "    token_list = [pair[0] for pair in token_importance_pairs]  # List of tokens in the sentence\n",
        "    covered_groups = []  # Track covered token groups\n",
        "    matched_positions = set()  # Track matched token indices\n",
        "\n",
        "    # Match each token group (unordered)\n",
        "    for group in token_groups:\n",
        "        print(f\"Checking Token Group: {group}\")\n",
        "        for start_idx in range(len(token_list) - len(group) + 1):\n",
        "            # Check if the group matches at any position in the token list\n",
        "            if token_list[start_idx:start_idx + len(group)] == group:\n",
        "                # Add the importance of the matched tokens\n",
        "                phrase_importance_sum += sum(\n",
        "                    abs(token_importance_pairs[start_idx + offset][1]) for offset in range(len(group))\n",
        "                )\n",
        "                covered_groups.append(group)  # Mark the group as covered\n",
        "                # Mark matched positions\n",
        "                matched_positions.update(range(start_idx, start_idx + len(group)))\n",
        "                print(f\"  Match Found for Group: {group} at Indices {list(range(start_idx, start_idx + len(group)))}\")\n",
        "                break\n",
        "        else:\n",
        "            print(f\"  No Match Found for Group: {group}\")\n",
        "\n",
        "    # Determine unmatched groups\n",
        "    unmatched_groups = [group for group in token_groups if group not in covered_groups]\n",
        "\n",
        "    # Identify remaining unmatched tokens\n",
        "    unmatched_tokens = [\n",
        "        token_list[idx] for idx in range(len(token_list)) if idx not in matched_positions\n",
        "    ]\n",
        "\n",
        "    # Return the metric, unmatched groups, and unmatched tokens\n",
        "    metric = phrase_importance_sum / total_importance if total_importance > 0 else 0\n",
        "    return metric, unmatched_groups, unmatched_tokens\n",
        "\n",
        "# Process each sentence and calculate the metric\n",
        "for idx, (sentence, token_importance_pairs) in enumerate(token_importance_results):\n",
        "    token_groups = token_groups_list[idx]  # Token groups for this sentence\n",
        "    metric, unmatched_groups, unmatched_tokens = calculate_phrase_metric_unordered(token_importance_pairs, token_groups)\n",
        "\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    print(f\"Tokenized Sentence: {[pair[0] for pair in token_importance_pairs]}\")\n",
        "    print(f\"Token Groups: {token_groups}\")\n",
        "    print(f\"Metric: {metric:.4f}\")\n",
        "    if not unmatched_groups:\n",
        "        print(\"All token groups were successfully matched and covered!\")\n",
        "    else:\n",
        "        print(\"Some token groups were not matched.\")\n",
        "        print(f\"Unmatched Token Groups: {unmatched_groups}\")\n",
        "        print(f\"Unmatched Tokens in Sentence: {unmatched_tokens}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqKfJ8ytxAyu",
        "outputId": "ffa6768d-fe18-47bc-f67d-e719368bab0e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at lyeonii/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking Token Group: ['stupid']\n",
            "  Match Found for Group: ['stupid'] at Indices [19]\n",
            "Checking Token Group: ['clown']\n",
            "  Match Found for Group: ['clown'] at Indices [2]\n",
            "\n",
            "Sentence: Another clown in favour of more tax in this country. Blows my mind people can be this stupid.\n",
            "Tokenized Sentence: ['[CLS]', 'another', 'clown', 'in', 'favour', 'of', 'more', 'tax', 'in', 'this', 'country', '.', 'blows', 'my', 'mind', 'people', 'can', 'be', 'this', 'stupid', '.', '[SEP]']\n",
            "Token Groups: [['stupid'], ['clown']]\n",
            "Metric: 0.0460\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['sex']\n",
            "  Match Found for Group: ['sex'] at Indices [1]\n",
            "\n",
            "Sentence: Sex in prison ought to be interesting for him\n",
            "Tokenized Sentence: ['[CLS]', 'sex', 'in', 'prison', 'ought', 'to', 'be', 'interesting', 'for', 'him', '[SEP]']\n",
            "Token Groups: [['sex']]\n",
            "Metric: 0.0623\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['the', 'mor', '##on']\n",
            "  Match Found for Group: ['the', 'mor', '##on'] at Indices [3, 4, 5]\n",
            "\n",
            "Sentence: DÃ¼stin \"the moron\" Darden can't even spell \"floride\" let alone understand it.\n",
            "Tokenized Sentence: ['[CLS]', 'dustin', '\"', 'the', 'mor', '##on', '\"', 'dar', '##den', 'can', \"'\", 't', 'even', 'spell', '\"', 'fl', '##ori', '##de', '\"', 'let', 'alone', 'understand', 'it', '.', '[SEP]']\n",
            "Token Groups: [['the', 'mor', '##on']]\n",
            "Metric: 0.0408\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['stupid']\n",
            "  Match Found for Group: ['stupid'] at Indices [14]\n",
            "Checking Token Group: ['ass']\n",
            "  Match Found for Group: ['ass'] at Indices [41]\n",
            "Checking Token Group: ['jerk']\n",
            "  Match Found for Group: ['jerk'] at Indices [68]\n",
            "Checking Token Group: ['worked', 'her', 'ass', 'off']\n",
            "  Match Found for Group: ['worked', 'her', 'ass', 'off'] at Indices [39, 40, 41, 42]\n",
            "\n",
            "Sentence: nope, they were still supporting a democrat.  it seems awfully stupid to allow more than one candidate to run in the primary but you cannot support one that the majority supports.  she may have worked her ass off but her behavior on national tv at the convention was not warranted.  people with any class and integrity do not behave like a jerk.\n",
            "Tokenized Sentence: ['[CLS]', 'nope', ',', 'they', 'were', 'still', 'supporting', 'a', 'democrat', '.', 'it', 'seems', 'awful', '##ly', 'stupid', 'to', 'allow', 'more', 'than', 'one', 'candidate', 'to', 'run', 'in', 'the', 'primary', 'but', 'you', 'cannot', 'support', 'one', 'that', 'the', 'majority', 'supports', '.', 'she', 'may', 'have', 'worked', 'her', 'ass', 'off', 'but', 'her', 'behavior', 'on', 'national', 'tv', 'at', 'the', 'convention', 'was', 'not', 'warrant', '##ed', '.', 'people', 'with', 'any', 'class', 'and', 'integrity', 'do', 'not', 'behave', 'like', 'a', 'jerk', '.', '[SEP]']\n",
            "Token Groups: [['stupid'], ['ass'], ['jerk'], ['worked', 'her', 'ass', 'off']]\n",
            "Metric: 0.1062\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "\n",
            "Sentence: naked app and Trump are the liars.\n",
            "\n",
            "The accusations are supported and over a week ago released in a report.\n",
            "Tokenized Sentence: ['[CLS]', 'naked', 'app', 'and', 'trump', 'are', 'the', 'liar', '##s', '.', 'the', 'accusations', 'are', 'supported', 'and', 'over', 'a', 'week', 'ago', 'released', 'in', 'a', 'report', '.', '[SEP]']\n",
            "Token Groups: []\n",
            "Metric: 0.0000\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['lovers']\n",
            "  Match Found for Group: ['lovers'] at Indices [9]\n",
            "\n",
            "Sentence: So you're advocating a list of lovers for Trump...I doubt if his wife will approve that, but they're a screwy couple.\n",
            "Tokenized Sentence: ['[CLS]', 'so', 'you', \"'\", 're', 'advocating', 'a', 'list', 'of', 'lovers', 'for', 'trump', '.', '.', '.', 'i', 'doubt', 'if', 'his', 'wife', 'will', 'approve', 'that', ',', 'but', 'they', \"'\", 're', 'a', 'screw', '##y', 'couple', '.', '[SEP]']\n",
            "Token Groups: [['lovers']]\n",
            "Metric: 0.0057\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['he', \"'\", 's', 'black']\n",
            "  Match Found for Group: ['he', \"'\", 's', 'black'] at Indices [23, 24, 25, 26]\n",
            "\n",
            "Sentence: The stock market increased more under Obama.  A lot more.  Did you thank him?\n",
            "\n",
            "Oh wait ... he's black, so no.\n",
            "Tokenized Sentence: ['[CLS]', 'the', 'stock', 'market', 'increased', 'more', 'under', 'obama', '.', 'a', 'lot', 'more', '.', 'did', 'you', 'thank', 'him', '?', 'oh', 'wait', '.', '.', '.', 'he', \"'\", 's', 'black', ',', 'so', 'no', '.', '[SEP]']\n",
            "Token Groups: [['he', \"'\", 's', 'black']]\n",
            "Metric: 0.0644\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['silly']\n",
            "  Match Found for Group: ['silly'] at Indices [4]\n",
            "\n",
            "Sentence: That is a silly argument.  Individually you pay no more or no less than anyone else based on federal tax rates.    Yes, thanks to the natural resources you have higher earning individuals which should be a positive thing, coupled with a younger work force that equates to more taxes going to the country....  somehow to the low information right that is negative (standard right wing mindset) and you choose to complain and whine about it.....so very sad.\n",
            "Tokenized Sentence: ['[CLS]', 'that', 'is', 'a', 'silly', 'argument', '.', 'individually', 'you', 'pay', 'no', 'more', 'or', 'no', 'less', 'than', 'anyone', 'else', 'based', 'on', 'federal', 'tax', 'rates', '.', 'yes', ',', 'thanks', 'to', 'the', 'natural', 'resources', 'you', 'have', 'higher', 'earning', 'individuals', 'which', 'should', 'be', 'a', 'positive', 'thing', ',', 'coupled', 'with', 'a', 'younger', 'work', 'force', 'that', 'e', '##qua', '##tes', 'to', 'more', 'taxes', 'going', 'to', 'the', 'country', '.', '.', '.', '.', 'somehow', 'to', 'the', 'low', 'information', 'right', 'that', 'is', 'negative', '(', 'standard', 'right', 'wing', 'minds', '##et', ')', 'and', 'you', 'choose', 'to', 'complain', 'and', 'w', '##hine', 'about', 'it', '.', '.', '.', '.', '.', 'so', 'very', 'sad', '.', '[SEP]']\n",
            "Token Groups: [['silly']]\n",
            "Metric: 0.0081\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['losers']\n",
            "  Match Found for Group: ['losers'] at Indices [2]\n",
            "Checking Token Group: ['sore', 'losers']\n",
            "  Match Found for Group: ['sore', 'losers'] at Indices [1, 2]\n",
            "\n",
            "Sentence: Sore losers is all this amount to.\n",
            "Tokenized Sentence: ['[CLS]', 'sore', 'losers', 'is', 'all', 'this', 'amount', 'to', '.', '[SEP]']\n",
            "Token Groups: [['losers'], ['sore', 'losers']]\n",
            "Metric: 0.1465\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['shit']\n",
            "  Match Found for Group: ['shit'] at Indices [3]\n",
            "\n",
            "Sentence: What a shit show.\n",
            "Tokenized Sentence: ['[CLS]', 'what', 'a', 'shit', 'show', '.', '[SEP]']\n",
            "Token Groups: [['shit']]\n",
            "Metric: 0.0800\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['stupid']\n",
            "  Match Found for Group: ['stupid'] at Indices [32]\n",
            "Checking Token Group: ['lunatic', 'liberals']\n",
            "  Match Found for Group: ['lunatic', 'liberals'] at Indices [7, 8]\n",
            "\n",
            "Sentence: clearly you are one of the lunatic liberals,,you cant refute or debate my notion that government is the problem thus proving you are the reject and stupid to boot!\n",
            "Tokenized Sentence: ['[CLS]', 'clearly', 'you', 'are', 'one', 'of', 'the', 'lunatic', 'liberals', ',', ',', 'you', 'can', '##t', 'ref', '##ute', 'or', 'debate', 'my', 'notion', 'that', 'government', 'is', 'the', 'problem', 'thus', 'proving', 'you', 'are', 'the', 'reject', 'and', 'stupid', 'to', 'boot', '!', '[SEP]']\n",
            "Token Groups: [['stupid'], ['lunatic', 'liberals']]\n",
            "Metric: 0.0445\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['stupid']\n",
            "  Match Found for Group: ['stupid'] at Indices [18]\n",
            "\n",
            "Sentence: Haha, I thought the whole point of chemtrails was to keep people stupid, so they'd vote for..... wait for it....\n",
            "Tokenized Sentence: ['[CLS]', 'ha', '##ha', ',', 'i', 'thought', 'the', 'whole', 'point', 'of', 'che', '##mt', '##rail', '##s', 'was', 'to', 'keep', 'people', 'stupid', ',', 'so', 'they', \"'\", 'd', 'vote', 'for', '.', '.', '.', '.', '.', 'wait', 'for', 'it', '.', '.', '.', '.', '[SEP]']\n",
            "Token Groups: [['stupid']]\n",
            "Metric: 0.0298\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['clown', '##s']\n",
            "  Match Found for Group: ['clown', '##s'] at Indices [12, 13]\n",
            "Checking Token Group: ['hell']\n",
            "  Match Found for Group: ['hell'] at Indices [19]\n",
            "\n",
            "Sentence: lol, adults leading now?  A bigger bunch of clowns I have never seen.  Hell the administration is not even consistent. (except for lying)\n",
            "\n",
            "Be proud republicans.  I know Putin is.   \n",
            "\n",
            "I just wonder how republicans feel  that their hero is  the guy supported by Putin?  I frankly don't know which is worse, to support a corrupt leader of a foreign country or to consider them the evil empire?  That is the thing I love about republicans, they go from one extreme  to the other  lol\n",
            "Tokenized Sentence: ['[CLS]', 'lo', '##l', ',', 'adults', 'leading', 'now', '?', 'a', 'bigger', 'bunch', 'of', 'clown', '##s', 'i', 'have', 'never', 'seen', '.', 'hell', 'the', 'administration', 'is', 'not', 'even', 'consistent', '.', '(', 'except', 'for', 'lying', ')', 'be', 'proud', 'republicans', '.', 'i', 'know', 'putin', 'is', '.', 'i', 'just', 'wonder', 'how', 'republicans', 'feel', 'that', 'their', 'hero', 'is', 'the', 'guy', 'supported', 'by', 'putin', '?', 'i', 'frankly', 'don', \"'\", 't', 'know', 'which', 'is', 'worse', ',', 'to', 'support', 'a', 'corrupt', 'leader', 'of', 'a', 'foreign', 'country', 'or', 'to', 'consider', 'them', 'the', 'evil', 'empire', '?', 'that', 'is', 'the', 'thing', 'i', 'love', 'about', 'republicans', ',', 'they', 'go', 'from', 'one', 'extreme', 'to', 'the', 'other', 'lo', '##l', '[SEP]']\n",
            "Token Groups: [['clown', '##s'], ['hell']]\n",
            "Metric: 0.0273\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "\n",
            "Sentence: The left is utterly clueless when it comes to masculinity. They are mostly comprised of homosexuals and atheists who don't even comprehend the values and qualities that REAL MEN have.  The fact is that these commie traitors have an agenda to feminize anything male or masculine and promote and defend every type of crime and sexual perversion known to humanity and for that reason alone, NOTHING that they say or do can ever be trusted. They must be PURGED from the Education System and I would not disagree to an open ended rifle season for hunting leftists everywhere.\n",
            "Tokenized Sentence: ['[CLS]', 'the', 'left', 'is', 'utterly', 'clue', '##less', 'when', 'it', 'comes', 'to', 'mas', '##cu', '##lini', '##ty', '.', 'they', 'are', 'mostly', 'comprised', 'of', 'homosexual', '##s', 'and', 'atheist', '##s', 'who', 'don', \"'\", 't', 'even', 'comprehend', 'the', 'values', 'and', 'qualities', 'that', 'real', 'men', 'have', '.', 'the', 'fact', 'is', 'that', 'these', 'com', '##mie', 'traitor', '##s', 'have', 'an', 'agenda', 'to', 'fe', '##mini', '##ze', 'anything', 'male', 'or', 'masculine', 'and', 'promote', 'and', 'defend', 'every', 'type', 'of', 'crime', 'and', 'sexual', 'per', '##version', 'known', 'to', 'humanity', 'and', 'for', 'that', 'reason', 'alone', ',', 'nothing', 'that', 'they', 'say', 'or', 'do', 'can', 'ever', 'be', 'trusted', '.', 'they', 'must', 'be', 'purge', '##d', 'from', 'the', 'education', 'system', 'and', 'i', 'would', 'not', 'disagree', 'to', 'an', 'open', 'ended', 'rifle', 'season', 'for', 'hunting', 'leftist', '##s', 'everywhere', '.', '[SEP]']\n",
            "Token Groups: []\n",
            "Metric: 0.0000\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n",
            "Checking Token Group: ['sound', 'like', 'a', 'third', 'rate', 'country', 'music', 'tavern', 'singer']\n",
            "  Match Found for Group: ['sound', 'like', 'a', 'third', 'rate', 'country', 'music', 'tavern', 'singer'] at Indices [9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
            "Checking Token Group: ['in', '##ept']\n",
            "  Match Found for Group: ['in', '##ept'] at Indices [35, 36]\n",
            "\n",
            "Sentence: Tugging at heartstrings?\n",
            "You sound like a third rate country music tavern singer pounding out third rate songs.\n",
            "Collect your lobbying pay some other way, you're inept at this method.\n",
            "Tokenized Sentence: ['[CLS]', 'tugging', 'at', 'hearts', '##tri', '##ng', '##s', '?', 'you', 'sound', 'like', 'a', 'third', 'rate', 'country', 'music', 'tavern', 'singer', 'pounding', 'out', 'third', 'rate', 'songs', '.', 'collect', 'your', 'lobbying', 'pay', 'some', 'other', 'way', ',', 'you', \"'\", 're', 'in', '##ept', 'at', 'this', 'method', '.', '[SEP]']\n",
            "Token Groups: [['sound', 'like', 'a', 'third', 'rate', 'country', 'music', 'tavern', 'singer'], ['in', '##ept']]\n",
            "Metric: 0.1222\n",
            "All token groups were successfully matched and covered!\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model_on_dataset(model_name, dataset):\n",
        "    \"\"\"\n",
        "    Evaluate a model on a dataset using the custom metric for token group coverage.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the model to evaluate.\n",
        "        dataset: The dataset to evaluate on (assumed to have 'test' split).\n",
        "\n",
        "    Returns:\n",
        "        float: The average metric value across all examples in the test split.\n",
        "    \"\"\"\n",
        "    # Filter examples with non-empty 'text_of_post' field\n",
        "    test_split = dataset['test']\n",
        "    valid_examples = [ex for ex in test_split if ex['text_of_post']]\n",
        "\n",
        "    # Extract examples and token groups\n",
        "    examples = [ex['text_of_post'] for ex in valid_examples]\n",
        "    labels = [ex['toxic'] for ex in valid_examples]\n",
        "\n",
        "    # Initialize model and tokenizer\n",
        "    model, tokenizer, tokenized_sentence, batch_lengths = load_model_and_tokenizer(model_name, examples)\n",
        "\n",
        "    token_groups_list = []\n",
        "    for ex in valid_examples:\n",
        "        text_dict = ast.literal_eval(ex['text'])\n",
        "        tokenized_groups = [tokenizer.tokenize(phrase) for phrase in text_dict.keys()]\n",
        "        token_groups_list.append(tokenized_groups)\n",
        "\n",
        "    # Calculate metrics for each example\n",
        "    metrics = []\n",
        "\n",
        "    for i in range(math.ceil(len(examples)/10)):\n",
        "        start_idx = i * 10\n",
        "        end_idx = min((i + 1) * 10, len(examples))\n",
        "        # Fetch token importances\n",
        "        token_importance_results = get_token_importance_for_sentences(model, tokenizer, examples[start_idx:end_idx], batch_lengths, labels[start_idx:end_idx])\n",
        "\n",
        "        # Function to calculate the metric for unordered token groups\n",
        "        def calculate_phrase_metric_unordered(token_importance_pairs, token_groups):\n",
        "            \"\"\"\n",
        "            Calculate the metric for token groups in a sentence considering unordered matches.\n",
        "\n",
        "            Args:\n",
        "                token_importance_pairs: List of (token, importance) pairs.\n",
        "                token_groups: List of tokenized groups (phrases) to match.\n",
        "\n",
        "            Returns:\n",
        "                float: The metric value for the token groups.\n",
        "            \"\"\"\n",
        "            phrase_importance_sum = 0\n",
        "            total_importance = sum(abs(importance) for _, importance in token_importance_pairs)\n",
        "            token_list = [pair[0] for pair in token_importance_pairs]\n",
        "            matched_positions = set()\n",
        "\n",
        "            # Match each token group\n",
        "            for group in token_groups:\n",
        "                for start_idx in range(len(token_list) - len(group) + 1):\n",
        "                    if token_list[start_idx:start_idx + len(group)] == group:\n",
        "                        phrase_importance_sum += sum(\n",
        "                            abs(token_importance_pairs[start_idx + offset][1]) for offset in range(len(group))\n",
        "                        )\n",
        "                        matched_positions.update(range(start_idx, start_idx + len(group)))\n",
        "                        break\n",
        "\n",
        "            metric = phrase_importance_sum / total_importance if total_importance > 0 else 0\n",
        "            return metric\n",
        "\n",
        "\n",
        "        for idx, (sentence, token_importance_pairs) in tqdm(\n",
        "            enumerate(token_importance_results),\n",
        "            total=len(token_importance_results),\n",
        "            desc=\"Processing Examples\"\n",
        "        ):\n",
        "            token_groups = token_groups_list[idx]\n",
        "            metric = calculate_phrase_metric_unordered(token_importance_pairs, token_groups)\n",
        "            metrics.append(metric)\n",
        "\n",
        "    # Return the average metric\n",
        "    average_metric = sum(metrics) / len(metrics) if metrics else 0\n",
        "    return average_metric\n",
        "\n",
        "# Example usage\n",
        "average_metric = evaluate_model_on_dataset(\"lyeonii/bert-tiny\", dataset)\n",
        "print(f\"Average Metric: {average_metric:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPIlNtYYHu3I",
        "outputId": "b5aca90b-857f-49c2-848c-608d47b914d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:349: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at lyeonii/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 3267.36it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 11413.07it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 13600.21it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6062.89it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 11745.46it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8839.42it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 13099.01it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 12139.81it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 4706.36it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8698.27it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 3505.77it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8248.39it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 3884.69it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6890.59it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 7817.90it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8179.22it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6995.17it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 7528.82it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 18608.27it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 16770.51it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 9374.84it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6108.80it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 11722.48it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 3410.28it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 13413.19it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 2082.57it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8279.32it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 10277.64it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 4135.17it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6503.81it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 11795.01it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8621.39it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 9706.79it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 7605.27it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 7055.18it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 10751.87it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 10599.71it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 16282.24it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 19793.79it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 5045.48it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6437.92it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8865.58it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 15224.33it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 3410.56it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 7866.29it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 9226.36it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 12038.76it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 7112.61it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8941.17it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 13252.15it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 18331.75it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 5058.25it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 5433.04it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 5056.42it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 10250.01it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 4471.06it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 4292.60it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 13560.63it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8067.52it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 11831.61it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 10285.20it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 10389.66it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 2722.69it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 10626.56it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 4788.56it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 7346.83it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 9433.88it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 9600.15it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 5268.56it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 7901.85it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 14716.86it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 18047.78it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6258.29it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6451.78it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 5082.16it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 11970.05it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 9633.22it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 12231.86it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 13366.17it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6617.71it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6775.94it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8987.15it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8391.96it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 10128.72it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 9287.65it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6000.43it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 4191.37it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 11008.67it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 16750.42it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8417.23it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 3956.14it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 9482.94it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 14583.81it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6786.90it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6098.14it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 9979.31it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 14753.09it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 6802.31it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 8232.20it/s]\n",
            "Processing Examples: 100%|ââââââââââ| 10/10 [00:00<00:00, 4875.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metric: 0.0057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = dataset['train'][10:15]['text_of_post']\n",
        "labels = dataset['train'][10:15]['toxic']\n",
        "\n",
        "model_name = \"lyeonii/bert-tiny\"\n",
        "model, tokenizer, tokenized_sentence, batch_lengths = load_model_and_tokenizer(model_name, examples)\n",
        "\n",
        "# Fetch token importances\n",
        "token_importance_results = get_token_importance_for_sentences(model, tokenizer, examples, batch_lengths, labels)\n",
        "\n",
        "# Print token importances for each sentence\n",
        "for sentence, token_importance_pairs in token_importance_results:\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    for token, importance in token_importance_pairs:\n",
        "        print(f\"  Token: {token}, Importance: {importance:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Token: {token}, Importance: {importance:.4f}\")"
      ],
      "metadata": {
        "id": "gcIqHLL1jzGh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}